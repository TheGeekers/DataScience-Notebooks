{"cells":[{"cell_type":"markdown","id":"e1adbca1-e4c7-420d-9b73-7c2a9324545b","metadata":{},"source":["<!-- <p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p> -->\n"]},{"cell_type":"markdown","id":"f1129203-8db3-424b-bb0d-03e1f432aaeb","metadata":{},"source":["# **Getting Started With Spark using Python**\n"]},{"cell_type":"markdown","id":"a68b76ca-9190-439c-9e11-5df810017fb9","metadata":{},"source":["![](http://spark.apache.org/images/spark-logo.png)\n"]},{"cell_type":"markdown","id":"d9aa96b4-a1a3-4a47-9d62-2da71280c951","metadata":{},"source":["### The Python API\n"]},{"cell_type":"markdown","id":"2331b9fd-6b9f-41b7-8c35-758355b3ad4a","metadata":{},"source":["Spark is written in Scala, which compiles to Java bytecode, but you can write python code to communicate to the java virtual machine through a library called py4j. Python has the richest API, but it can be somewhat limiting if you need to use a method that is not available, or if you need to write a specialized piece of code. The latency associated with communicating back and forth to the JVM can sometimes cause the code to run slower.\n","An exception to this is the SparkSQL library, which has an execution planning engine that precompiles the queries. Even with this optimization, there are cases where the code may run slower than the native scala version.\n","The general recommendation for PySpark code is to use the \"out of the box\" methods available as much as possible and avoid overly frequent (iterative) calls to Spark methods. If you need to write high-performance or specialized code, try doing it in scala.\n","But hey, we know Python rules, and the plotting libraries are way better. So, it's up to you!\n"]},{"cell_type":"markdown","id":"ab8f0e24-eb31-44f2-9eff-164ffaf63cd7","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"7e569f93-57c5-4637-a720-d4dbac774cd6","metadata":{},"source":["In this Notebook, we will go over the basics of Apache Spark and PySpark. We will start with creating the SparkContext and SparkSession. We then create an RDD and apply some basic transformations and actions. Finally we demonstrate the basics dataframes and SparkSQL.\n","\n","After this Notebook you will be able to:\n","\n","* Create the SparkContext and SparkSession\n","* Create an RDD and apply some basic transformations and actions to RDDs\n","* Demonstrate the use of the basics Dataframes and SparkSQL\n"]},{"cell_type":"markdown","id":"c834b6a3-1df9-4fae-9cfd-c539d2eb8ad4","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"62398c88-7449-4b9c-a250-16bc9fb2dbde","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"d4aba01f-d6bd-4876-b264-81f8091fb7d1","metadata":{},"source":["For this lab, we are going to be using Python and Spark (PySpark). These libraries should be installed in your lab environment or in SN Labs.\n"]},{"cell_type":"code","execution_count":20,"id":"76bf9701-24e2-4baf-beda-3c4024c1ee87","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /home/mehran/Projects/BigDataHadop&Spark/.venv/lib/python3.11/site-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /home/mehran/Projects/BigDataHadop&Spark/.venv/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: findspark in /home/mehran/Projects/BigDataHadop&Spark/.venv/lib/python3.11/site-packages (2.0.1)\n"]}],"source":["# Installing required packages\n","!pip install pyspark\n","!pip install findspark"]},{"cell_type":"code","execution_count":21,"id":"80729507-54c2-4544-b596-04c49266bd57","metadata":{},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":22,"id":"e5fe8095-f25c-4a63-87f0-5a9a555f3dc5","metadata":{},"outputs":[],"source":["# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","id":"6d6176b6-aa1d-4bfc-98d6-3e25481025c2","metadata":{},"source":["## Spark Context and Spark Session\n"]},{"cell_type":"markdown","id":"55e322fd-f9df-4acc-8a43-4b425c14b7c9","metadata":{},"source":["In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n","SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"]},{"cell_type":"markdown","id":"daa7a2a5-bd1f-499a-bb49-a46966399b98","metadata":{},"source":["####  Creating the spark session and context\n"]},{"cell_type":"code","execution_count":null,"id":"cbae5965-c4d9-4ad0-8044-205078a1ca0b","metadata":{},"outputs":[],"source":["# Creating a spark context class\n","sc = SparkContext()\n","\n","# Creating a spark session\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"]},{"cell_type":"markdown","id":"fc8a3e74-7cb8-44a5-a04a-f5724a909b21","metadata":{},"source":["####  Initialize Spark session\n","To work with dataframes we just need to verify that the spark session instance has been created.\n"]},{"cell_type":"code","execution_count":null,"id":"33c49fa7-8663-4ed5-ab13-890af7d375cb","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://192.168.1.106:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fb17c51ad10>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","id":"5d04702a-0465-4556-9411-2f2faef6c412","metadata":{},"source":["##  RDDs\n","In this exercise we work with Resilient Distributed Datasets (RDDs). RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs. \n"]},{"cell_type":"markdown","id":"c5116e0c-0f28-47d5-ba29-d6abfaf472c1","metadata":{},"source":["####  Create an RDD.\n","For demonstration purposes, we create an RDD here by calling `sc.parallelize()`  \n","We create an RDD which has integers from 1 to 30.\n"]},{"cell_type":"code","execution_count":null,"id":"1457d85b-c6fc-49da-9fcf-4d52030d6604","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["length of Data is 99\n"]},{"data":{"text/plain":["PythonRDD[1] at RDD at PythonRDD.scala:53"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data = range(1,100)\n","# print first element of iterator\n","print(\"length of Data is\",len(data))\n","xrangeRDD = sc.parallelize(data, len(data))\n","\n","# this will let us know that we created an RDD\n","xrangeRDD"]},{"cell_type":"markdown","id":"14531fcf-247c-477c-82a3-4ff8768827bb","metadata":{},"source":["####  Transformations\n"]},{"cell_type":"markdown","id":"3a4a960c-52a7-4558-aa1b-d6c87798a8d7","metadata":{},"source":["A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. In this transformation, we reduce each element in the RDD by 1. Note the use of the lambda function. We also then filter the RDD to only contain elements <10.\n"]},{"cell_type":"code","execution_count":null,"id":"78e92a9f-e02b-42e4-a108-e2995e23de26","metadata":{},"outputs":[],"source":["subRDD = xrangeRDD.map(lambda x: x-1)\n","filteredRDD = subRDD.filter(lambda x : x<50)"]},{"cell_type":"markdown","id":"30c60936-0fe0-411a-889d-3ab4819ed93e","metadata":{},"source":["####  Actions \n"]},{"cell_type":"markdown","id":"44d8a054-d867-4e17-8acd-ee769f360297","metadata":{},"source":["A transformation returns a result to the driver. We now apply the `collect()` action to get the output from the transformation.\n"]},{"cell_type":"code","execution_count":null,"id":"9a0bf8de-290b-47a2-a05b-76d1ca888094","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["50"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["print(filteredRDD.collect())\n","filteredRDD.count()"]},{"cell_type":"markdown","id":"cc887de8-0a0f-401d-8217-a768b67d5f93","metadata":{},"source":["####  Caching Data\n"]},{"cell_type":"markdown","id":"76cc03c8-986e-41ff-956a-4ff47151beb3","metadata":{},"source":["This simple example shows how to create an RDD and cache it. Notice the **10x speed improvement**!  If you wish to see the actual computation time, browse to the Spark UI...it's at `localhost:4040`.  You'll see that the second calculation took much less time!\n"]},{"cell_type":"code","execution_count":null,"id":"7e92825a-9196-4a49-971f-92d480fb2f33","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["dt1:  0.6610550880432129\n","dt2:  0.3174583911895752\n"]}],"source":["import time \n","\n","test = sc.parallelize(range(1,50000),4)\n","test.cache()\n","\n","t1 = time.time()\n","# first count will trigger evaluation of count *and* cache\n","count1 = test.count()\n","dt1 = time.time() - t1\n","print(\"dt1: \", dt1)\n","\n","\n","t2 = time.time()\n","# second count operates on cached data only\n","count2 = test.count()\n","dt2 = time.time() - t2\n","print(\"dt2: \", dt2)\n","\n","#test.count()"]},{"cell_type":"markdown","id":"99269f34-75f2-46f5-9950-3464b9cfd513","metadata":{},"source":["##  DataFrames and SparkSQL\n"]},{"cell_type":"markdown","id":"0b8238d6-a35a-493e-8a1e-fb2cfcf22242","metadata":{},"source":["In order to work with the extremely powerful SQL engine in Apache Spark, you will need a Spark Session. We have created that in the first Exercise, let us verify that spark session is still active.\n"]},{"cell_type":"code","execution_count":null,"id":"616e4f70-c6cd-4d00-b4bb-2b1a5b7b50d2","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://192.168.1.106:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fb17c51ad10>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","id":"feb6a37c-cfed-490a-a629-5d836211c90e","metadata":{},"source":["####  Create Your First DataFrame!\n"]},{"cell_type":"markdown","id":"4cb5dbfe-848c-4f0f-8193-a35c2b4c54f9","metadata":{},"source":["You can create a structured data set (much like a database table) in Spark.  Once you have done that, you can then use powerful SQL tools to query and join your dataframes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame.\n","df = spark.read.option(\"header\", True).csv(\"Datasets/us-500.csv\").cache()"]},{"cell_type":"code","execution_count":24,"id":"5cd6abb7-7c4c-4287-8f60-c7b9d5d98fb8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+------------+------------+--------------------+--------------------+\n","|first_name|last_name|        company_name|             address|         city|        county|state|  zip|      phone1|      phone2|               email|                 web|\n","+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+------------+------------+--------------------+--------------------+\n","|     James|     Butt|   Benton, John B Jr|  6649 N Blue Gum St|  New Orleans|       Orleans|   LA|70116|504-621-8927|504-845-1427|     jbutt@gmail.com|http://www.benton...|\n","| Josephine|  Darakjy|Chanay, Jeffrey A...| 4 B Blue Ridge Blvd|     Brighton|    Livingston|   MI|48116|810-292-9388|810-374-9840|josephine_darakjy...|http://www.chanay...|\n","|       Art|   Venere| Chemel, James L Cpa|8 W Cerritos Ave #54|   Bridgeport|    Gloucester|   NJ|08014|856-636-8749|856-264-4130|      art@venere.org|http://www.chemel...|\n","|     Lenna| Paprocki|Feltz Printing Se...|         639 Main St|    Anchorage|     Anchorage|   AK|99501|907-385-4412|907-921-2010|lpaprocki@hotmail...|http://www.feltzp...|\n","|   Donette|   Foller| Printing Dimensions|        34 Center St|     Hamilton|        Butler|   OH|45011|513-570-1893|513-549-4561|donette.foller@co...|http://www.printi...|\n","|    Simona|  Morasca| Chapman, Ross E Esq|        3 Mcauley Dr|      Ashland|       Ashland|   OH|44805|419-503-2484|419-800-6759|  simona@morasca.com|http://www.chapma...|\n","|    Mitsue|  Tollner|  Morlong Associates|           7 Eads St|      Chicago|          Cook|   IL|60632|773-573-6914|773-924-8565|mitsue_tollner@ya...|http://www.morlon...|\n","|     Leota| Dilliard|    Commercial Press|    7 W Jackson Blvd|     San Jose|   Santa Clara|   CA|95111|408-752-3500|408-813-1105|   leota@hotmail.com|http://www.commer...|\n","|      Sage|   Wieser|Truhlar And Truhl...|    5 Boston Ave #88|  Sioux Falls|     Minnehaha|   SD|57105|605-414-2147|605-794-4895| sage_wieser@cox.net|http://www.truhla...|\n","|      Kris|  Marrier|King, Christopher...|228 Runamuck Pl #...|    Baltimore|Baltimore City|   MD|21224|410-655-8723|410-804-4694|      kris@gmail.com|http://www.kingch...|\n","|     Minna|   Amigon|   Dorl, James J Esq|    2371 Jerrold Ave|   Kulpsville|    Montgomery|   PA|19443|215-874-1229|215-422-8694|minna_amigon@yaho...|http://www.dorlja...|\n","|      Abel|  Maclead| Rangoni Of Florence|  37275 St  Rt 17m M|Middle Island|       Suffolk|   NY|11953|631-335-3414|631-677-3675|  amaclead@gmail.com|http://www.rangon...|\n","|     Kiley|Caldarera|         Feiner Bros|    25 E 75th St #69|  Los Angeles|   Los Angeles|   CA|90034|310-498-5651|310-254-3084|kiley.caldarera@a...|http://www.feiner...|\n","|  Graciela|     Ruta|Buckley Miller & ...|98 Connecticut Av...|Chagrin Falls|        Geauga|   OH|44023|440-780-8425|440-579-7763|       gruta@cox.net|http://www.buckle...|\n","|     Cammy|  Albares|Rousseaux, Michae...|    56 E Morehead St|       Laredo|          Webb|   TX|78045|956-537-6195|956-841-7216|  calbares@gmail.com|http://www.rousse...|\n","|    Mattie| Poquette|Century Communica...| 73 State Road 434 E|      Phoenix|      Maricopa|   AZ|85013|602-277-4385|602-953-6360|      mattie@aol.com|http://www.centur...|\n","|   Meaghan|   Garufi|  Bolton, Wilbur Esq| 69734 E Carrillo St| Mc Minnville|        Warren|   TN|37110|931-313-9635|931-235-7959| meaghan@hotmail.com|http://www.bolton...|\n","|    Gladys|      Rim|T M Byxbee Compan...|322 New Horizon Blvd|    Milwaukee|     Milwaukee|   WI|53207|414-661-9598|414-377-2880|  gladys.rim@rim.org|http://www.tmbyxb...|\n","|      Yuki|  Whobrey|Farmers Insurance...|    1 State Route 27|       Taylor|         Wayne|   MI|48180|313-288-7937|313-341-4470|yuki_whobrey@aol.com|http://www.farmer...|\n","|  Fletcher|    Flosi|Post Box Services...| 394 Manchester Blvd|     Rockford|     Winnebago|   IL|61109|815-828-2147|815-426-5657|fletcher.flosi@ya...|http://www.postbo...|\n","+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+------------+------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n","root\n"," |-- first_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- company_name: string (nullable = true)\n"," |-- address: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- county: string (nullable = true)\n"," |-- state: string (nullable = true)\n"," |-- zip: string (nullable = true)\n"," |-- phone1: string (nullable = true)\n"," |-- phone2: string (nullable = true)\n"," |-- email: string (nullable = true)\n"," |-- web: string (nullable = true)\n","\n"]}],"source":["# Print the dataframe as well as the data schema\n","df.show()\n","df.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"395dec60-e558-4bce-aa1c-7c3227ab66b8","metadata":{},"outputs":[],"source":["# Register the DataFrame as a SQL temporary view\n","df.createTempView(\"us_500\")"]},{"cell_type":"markdown","id":"33bd769c-d2e6-476e-96eb-0d3a6bf7f548","metadata":{},"source":["####  Explore the data using DataFrame functions and SparkSQL\n","\n","In this section, we explore the datasets using functions both from dataframes as well as corresponding SQL queries using sparksql. Note the different ways to achieve the same task!\n"]},{"cell_type":"code","execution_count":null,"id":"a84f93e7-57bd-4dc5-8ebd-2dfe72bc1029","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|first_name|\n","+----------+\n","|     James|\n","| Josephine|\n","|       Art|\n","|     Lenna|\n","|   Donette|\n","|    Simona|\n","|    Mitsue|\n","|     Leota|\n","|      Sage|\n","|      Kris|\n","|     Minna|\n","|      Abel|\n","|     Kiley|\n","|  Graciela|\n","|     Cammy|\n","|    Mattie|\n","|   Meaghan|\n","|    Gladys|\n","|      Yuki|\n","|  Fletcher|\n","+----------+\n","only showing top 20 rows\n","\n","+----------+\n","|first_name|\n","+----------+\n","|     James|\n","| Josephine|\n","|       Art|\n","|     Lenna|\n","|   Donette|\n","|    Simona|\n","|    Mitsue|\n","|     Leota|\n","|      Sage|\n","|      Kris|\n","|     Minna|\n","|      Abel|\n","|     Kiley|\n","|  Graciela|\n","|     Cammy|\n","|    Mattie|\n","|   Meaghan|\n","|    Gladys|\n","|      Yuki|\n","|  Fletcher|\n","+----------+\n","only showing top 20 rows\n","\n"]}],"source":["# Select and show basic data columns\n","\n","df.select(\"first_name\").show() \n","# or\n","df.select(df[\"first_name\"]).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|first_name|\n","+----------+\n","|     James|\n","| Josephine|\n","|       Art|\n","|     Lenna|\n","|   Donette|\n","|    Simona|\n","|    Mitsue|\n","|     Leota|\n","|      Sage|\n","|      Kris|\n","|     Minna|\n","|      Abel|\n","|     Kiley|\n","|  Graciela|\n","|     Cammy|\n","|    Mattie|\n","|   Meaghan|\n","|    Gladys|\n","|      Yuki|\n","|  Fletcher|\n","+----------+\n","only showing top 20 rows\n","\n"]}],"source":["# Select and show basic data columns using sql\n","\n","spark.sql(\"SELECT first_name FROM us_500\").show()"]},{"cell_type":"code","execution_count":null,"id":"610b7065-1252-40aa-8ea4-47a6e5c90b2c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+--------------------+--------------------+-------+------+-----+-----+------------+------------+--------------------+--------------------+\n","|first_name|last_name|        company_name|             address|   city|county|state|  zip|      phone1|      phone2|               email|                 web|\n","+----------+---------+--------------------+--------------------+-------+------+-----+-----+------------+------------+--------------------+--------------------+\n","|    Mitsue|  Tollner|  Morlong Associates|           7 Eads St|Chicago|  Cook|   IL|60632|773-573-6914|773-924-8565|mitsue_tollner@ya...|http://www.morlon...|\n","|      Viva|  Toelkes|   Mark Iv Press Ltd|      4284 Dorigo Ln|Chicago|  Cook|   IL|60647|773-446-5569|773-352-3437|viva.toelkes@gmai...|http://www.markiv...|\n","|     Marti|  Maybury|Eldridge, Kristin...|4 Warehouse Point...|Chicago|  Cook|   IL|60638|773-775-4522|773-539-1058|marti.maybury@yah...|http://www.eldrid...|\n","|  Valentin|   Klimek|Schmid, Gayanne K...|     137 Pioneer Way|Chicago|  Cook|   IL|60604|312-303-5453|312-512-2338|  vklimek@klimek.org|http://www.schmid...|\n","|   Carmela|   Cookey|Royal Pontiac Old...|   9 Murfreesboro Rd|Chicago|  Cook|   IL|60623|773-494-4195|773-297-9391|  ccookey@cookey.org|http://www.royalp...|\n","|     Erick|   Nievas|    Soward, Anne Esq|      45 E Acacia Ct|Chicago|  Cook|   IL|60624|773-704-9903|773-359-6109|erick_nievas@aol.com|http://www.soward...|\n","|  Nichelle|   Meteer|        Print Doctor|    72 Beechwood Ter|Chicago|  Cook|   IL|60657|773-225-9985|773-857-2231|nichelle_meteer@m...|http://www.printd...|\n","+----------+---------+--------------------+--------------------+-------+------+-----+-----+------------+------------+--------------------+--------------------+\n","\n"]}],"source":["# Perform basic filtering using dataframe\n","df.filter(df[\"city\"] == \"Chicago\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-------+\n","|first_name|   city|\n","+----------+-------+\n","|    Mitsue|Chicago|\n","|      Viva|Chicago|\n","|     Marti|Chicago|\n","|  Valentin|Chicago|\n","|   Carmela|Chicago|\n","|     Erick|Chicago|\n","|  Nichelle|Chicago|\n","+----------+-------+\n","\n"]}],"source":["# Perform basic filtering using sql\n","\n","spark.sql(\"SELECT first_name, city  FROM us_500 WHERE city='Chicago' \").show()"]},{"cell_type":"code","execution_count":null,"id":"0956c603-be78-484e-af13-431c43ef6702","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+-----+\n","|         city|count|\n","+-------------+-----+\n","|    Fairbanks|    2|\n","|    Worcester|    3|\n","|      Hanover|    1|\n","|Bowling Green|    1|\n","|   Harrisburg|    1|\n","|     Palatine|    1|\n","|     Harrison|    1|\n","|      Phoenix|    5|\n","|   Plainfield|    1|\n","|  Cherry Hill|    2|\n","|  Cedar Grove|    1|\n","|   Perrysburg|    1|\n","|     Brighton|    1|\n","|   Toms River|    1|\n","|     Lynbrook|    1|\n","|        Omaha|    1|\n","|    Anchorage|    4|\n","|    Hampstead|    1|\n","|      Bothell|    1|\n","|      Anaheim|    1|\n","+-------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["# Perfom basic aggregation of data using dataframe\n","df.groupBy(\"city\").count().show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+-----+\n","|         city|count|\n","+-------------+-----+\n","|    Fairbanks|    2|\n","|    Worcester|    3|\n","|      Hanover|    1|\n","|Bowling Green|    1|\n","|   Harrisburg|    1|\n","|     Palatine|    1|\n","|     Harrison|    1|\n","|      Phoenix|    5|\n","|   Plainfield|    1|\n","|  Cherry Hill|    2|\n","|  Cedar Grove|    1|\n","|   Perrysburg|    1|\n","|     Brighton|    1|\n","|   Toms River|    1|\n","|     Lynbrook|    1|\n","|        Omaha|    1|\n","|    Anchorage|    4|\n","|    Hampstead|    1|\n","|      Bothell|    1|\n","|      Anaheim|    1|\n","+-------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["# Perfom basic aggregation of data using sql\n","spark.sql(\"SELECT city, COUNT(city) as count FROM us_500 GROUP BY city\").show()"]},{"cell_type":"markdown","id":"e99a8bf2-affe-4c10-b237-3c963d54fb5e","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"e482c21f-999b-4135-a15b-72a998ff48c9","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"4332896f-7510-4778-b3be-87432b1bf59f","metadata":{},"source":["[Mehran Morabbi Pazoki](https://www.linkedin.com/in/mehran-pazoki-6a3372175/)\n","\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
